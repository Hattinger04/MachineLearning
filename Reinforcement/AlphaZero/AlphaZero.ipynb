{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9ec190",
   "metadata": {},
   "source": [
    "# Alpha Zero\n",
    "\n",
    "This project is about developing my own chess AI using PyTorch.  \n",
    "While gathering information, Robert FÃ¶rster's course on YouTube (https://www.youtube.com/watch?v=wuSQpLinRB4) has been very informative and inspired me to getting starting with RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88d1e8",
   "metadata": {},
   "source": [
    "## Importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting gym==0.17.2\n",
      "  Downloading gym-0.17.2.tar.gz (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     -                                        0.1/1.6 MB 825.8 kB/s eta 0:00:02\n",
      "     -                                        0.1/1.6 MB 825.8 kB/s eta 0:00:02\n",
      "     --                                       0.1/1.6 MB 656.4 kB/s eta 0:00:03\n",
      "     ---                                      0.1/1.6 MB 554.9 kB/s eta 0:00:03\n",
      "     ---                                      0.1/1.6 MB 554.9 kB/s eta 0:00:03\n",
      "     ----                                     0.2/1.6 MB 492.3 kB/s eta 0:00:03\n",
      "     ----                                     0.2/1.6 MB 535.8 kB/s eta 0:00:03\n",
      "     ------                                   0.2/1.6 MB 576.2 kB/s eta 0:00:03\n",
      "     -------                                  0.3/1.6 MB 632.7 kB/s eta 0:00:03\n",
      "     --------                                 0.3/1.6 MB 677.0 kB/s eta 0:00:02\n",
      "     ----------                               0.4/1.6 MB 732.8 kB/s eta 0:00:02\n",
      "     -----------                              0.5/1.6 MB 779.5 kB/s eta 0:00:02\n",
      "     -------------                            0.5/1.6 MB 802.8 kB/s eta 0:00:02\n",
      "     ---------------                          0.6/1.6 MB 868.6 kB/s eta 0:00:02\n",
      "     ----------------                         0.6/1.6 MB 883.0 kB/s eta 0:00:02\n",
      "     ------------------                       0.7/1.6 MB 935.8 kB/s eta 0:00:01\n",
      "     --------------------                     0.8/1.6 MB 996.3 kB/s eta 0:00:01\n",
      "     ----------------------                   0.9/1.6 MB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------                 1.0/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------              1.1/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------             1.1/1.6 MB 1.1 MB/s eta 0:00:01\n",
      "     -----------------------------            1.1/1.6 MB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------------           1.2/1.6 MB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.6 MB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.6 MB 1.0 MB/s eta 0:00:01\n",
      "     --------------------------------         1.3/1.6 MB 999.2 kB/s eta 0:00:01\n",
      "     ---------------------------------        1.3/1.6 MB 982.9 kB/s eta 0:00:01\n",
      "     ----------------------------------       1.4/1.6 MB 983.0 kB/s eta 0:00:01\n",
      "     -----------------------------------      1.4/1.6 MB 972.2 kB/s eta 0:00:01\n",
      "     ------------------------------------     1.4/1.6 MB 962.0 kB/s eta 0:00:01\n",
      "     -------------------------------------    1.5/1.6 MB 952.4 kB/s eta 0:00:01\n",
      "     --------------------------------------   1.5/1.6 MB 947.4 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 958.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages (from gym==0.17.2) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages (from gym==0.17.2) (1.23.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages (from gym==0.17.2) (1.5.0)\n",
      "Collecting cloudpickle<1.4.0,>=1.2.0 (from gym==0.17.2)\n",
      "  Downloading cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: future in c:\\users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.2) (0.18.3)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650874 sha256=bbb81b5013798f5b42985fde4109cd95a71bab50d9317ecf75487a5e6d9d694d\n",
      "  Stored in directory: C:\\Users\\s8gre\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ru_ihz4_\\wheels\\48\\bf\\7c\\44b1b8e4ad998fc48e31caedbb9e028351861b8d20632642bc\n",
      "Successfully built gym\n",
      "Installing collected packages: cloudpickle, gym\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "Successfully installed cloudpickle-1.3.0 gym-0.17.2\n"
     ]
    }
   ],
   "source": [
    "%pip install gym==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd6456",
   "metadata": {},
   "source": [
    "## Create a game\n",
    "\n",
    "The game must have following properties / methods:\n",
    "\n",
    "- <b>row_count:</b> return the number of rows of the board\n",
    "- <b>column_count:</b> return the number of columns of the board\n",
    "- <b>action_size:</b> return the actions which can be taken at the board (TicTacToe: row_count \\* column_count)\n",
    "- <b>get_initial_state():</b> return the board at the beginning of the game\n",
    "- <b>get_next_state(state, action, player):</b> make a move and update the board\n",
    "- <b>get_valid_moves(state):</b> return all possible moves at the current state\n",
    "- <b>check_win(state, action):</b> check if an action at a current state results in a win\n",
    "- <b>get_value_and_terminated(state, action):</b> check if the game has ended in some way (lose, draw, win)\n",
    "- <b>get_opponent(player):</b> get the opponent player\n",
    "- <b>get_opponent_value(player):</b> get the value of the opponent player\n",
    "- <b>change_perspective(state, player):</b> change the perspective of the board (e.g. state \\* player)\n",
    "- <b>get_encoded_state(state):</b> return a encoded version of the current state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True # this action ended the game with a win for the player who took this action\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True # this action ended the game but the players drew\n",
    "        return 0, False # the game has not ended yet\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # check if batch dim exists\n",
    "        # if exists then correct the order\n",
    "        if len(state.shape) == 3: \n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "\n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99df032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True # this action ended the game with a win for the player who took this action\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True # this action ended the game but the players drew\n",
    "        return 0, False # the game has not ended yet\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # check if batch dim exists\n",
    "        # if exists then correct the order\n",
    "        if len(state.shape) == 3: \n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "\n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f4e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import gym, gym_chess\n",
    "import lib.move_encoding.functions as environment\n",
    "from lib.move_encoding.utils import rotate\n",
    "\n",
    "class Chess:\n",
    "    def __init__(self):\n",
    "        self.row_count = 8\n",
    "        self.column_count = 8\n",
    "        self.env = environment.MoveEncoding(gym.make('ChessAlphaZero-v0'))\n",
    "        self.action_size = self.env.action_space.n\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Chess\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.env.reset()\n",
    "        return chess.Board()\n",
    "    \n",
    "    def move_to_action(self, move, state):\n",
    "        return self.env.encode(move, state)\n",
    "\n",
    "    def action_to_move(self, action, state):\n",
    "        return self.env.decode(action, state)\n",
    "\n",
    "    # state equals board\n",
    "    def get_next_state(self, state, action, player):\n",
    "        move = self.action_to_move(action, state)\n",
    "        state.push(move)\n",
    "        return state\n",
    "\n",
    "        \n",
    "    def get_valid_moves(self, state):\n",
    "        legal_moves = np.zeros(self.action_size)\n",
    "        for move in state.legal_moves:\n",
    "            n_move = self.move_to_action(move, state)\n",
    "            legal_moves[n_move] = 1\n",
    "        return legal_moves\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        return state.is_checkmate()\n",
    "    \n",
    "        \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True # this action ended the game with a win for the player who took this action\n",
    "        if state.is_stalemate() or state.is_insufficient_material() or state.can_claim_fifty_moves() or state.can_claim_threefold_repetition():\n",
    "            return 0, True # this action ended the game but the players drew\n",
    "        return 0, False # the game has not ended yet\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    # Channels 0-5: White pieces (Pawn, Knight, Bishop, Rook, Queen, King).\n",
    "    # Channels 6-11: Black pieces (Pawn, Knight, Bishop, Rook, Queen, King).\n",
    "    # Channel 12: Side to move (1 for white, 0 for black).\n",
    "    # Channel 13-14: Castling rights for white (kingside, queenside).\n",
    "    # Channel 15-16: Castling rights for black (kingside, queenside).\n",
    "    # Channel 17: Potential en passant square (encoded as a binary matrix with a 1 on the potential en passant square, if any).\n",
    "    def get_encoded_state(self, state):\n",
    "       # Create an 8x8x18 feature plane\n",
    "        encoded_state = np.zeros((18, 8, 8), dtype=np.float32)\n",
    "        # Iterate through each square on the board\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                square = chess.square(col, 7 - row)  # Flip the row for the correct orientation\n",
    "                piece = state.piece_at(square)\n",
    "                if piece is not None:\n",
    "                    piece_index = chess.PIECE_TYPES.index(piece.piece_type)\n",
    "                    color_index = int(piece.color)\n",
    "                    channel = piece_index + color_index * 6  # 6 channels per piece type\n",
    "                    encoded_state[channel, row, col] = 1\n",
    "\n",
    "        # Encode side to move\n",
    "        encoded_state[12, :, :] = 1 if state.turn == chess.WHITE else 0\n",
    "        \n",
    "        # Encode castling rights\n",
    "        encoded_state[13, :, :] = int(state.has_kingside_castling_rights(chess.WHITE))\n",
    "        encoded_state[14, :, :] = int(state.has_queenside_castling_rights(chess.WHITE))\n",
    "        encoded_state[15, :, :] = int(state.has_kingside_castling_rights(chess.BLACK))\n",
    "        encoded_state[16, :, :] = int(state.has_queenside_castling_rights(chess.BLACK))\n",
    "\n",
    "        # Encode potential en passant square\n",
    "        if state.ep_square is not None:\n",
    "            row, col = 7 - state.ep_square // 8, state.ep_square % 8  # Extracting row, col from square number\n",
    "            encoded_state[17, row, col] = 1\n",
    "\n",
    "        # check if batch dim exists\n",
    "        # if exists then correct the order\n",
    "        # if len(state.shape) == 4: \n",
    "        #    encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "\n",
    "        return encoded_state        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c8b50",
   "metadata": {},
   "source": [
    "## Designing of a model\n",
    "\n",
    "Here a ResNet model has been chosen, containing of multiple Conv2D Layers which can find patterns in 2d games like TicTacToe or chess.  \n",
    "The ResNet has following layers:\n",
    "\n",
    "- 1 x startblock\n",
    "- n x backbone (here the input will be additionally added to the output)\n",
    "- 1 x policyhead (returns policy values as a linear layer)\n",
    "- 1 x valuehead (returns value in range -1 and 1 (by using tanh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, num_channels, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        # combination of kernel_size=3, padding=1 and stride=1 important => guarantees that input_shape = output_shape  \n",
    "        # e.g.: 5x5 image with 3x3 convolution results in a 3x3 output => padding 1 in all directions results in a 5x5 output\n",
    "        # therefore the shape of the image stays consistant during the computations\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_hidden, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "    \n",
    "\n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a7539",
   "metadata": {},
   "source": [
    "## Updated MCTS Implementation\n",
    "\n",
    "Using the Monte Carlo Tree Search algorithm enables splitting the game of choice in many different paths of action and their probability of success.  \n",
    "Therefore it is perfect for solving a game tree in games like chess or TicTacToe.  \n",
    "It contains of four phases:\n",
    "\n",
    "- <u>Selection:</u> Walking down until a leaf node has been found (leaf node is a node that could be expanded even further)  \n",
    "   The path is determined by the UCB-fomular (a node is chosen which has a higher winning prob and lower visit counts)\n",
    "- <u>Expansion:</u> When reached a leaf node in the selection phase, new leaf node(s) will be created, which are attached at the current node  \n",
    "   The number of leaf nodes are equal to the number of valid moves\n",
    "- <u>Simulation:</u> Simulate the game by either random choices or like in the updated version, by using a neural network\n",
    "- <u>Backpropagation:</u> Update the tree from the bottom to the top (until root node is reached)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b31e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state # current state of the game\n",
    "        self.parent = parent # the parent node\n",
    "        self.action_taken = action_taken # the action which has to be taken to arrive at this node \n",
    "        self.prior = prior # is the policy value the model provided us \n",
    "        \n",
    "        self.children = [] # list of all child nodes\n",
    "        \n",
    "        self.visit_count = visit_count # the count of all visits to this node\n",
    "        self.value_sum = 0 # the current sum of all simulations (of the neural network)\n",
    "    \n",
    "    # check if current node has child nodes\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    # returns the child node with the highest ucb score during selection phase\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    # calculate the ucb value of a child node\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # 1 - ()  because we want to have the opponent in the worst possible situation in the next move\n",
    "            # ( +1) / 2 because we want a proablilty and not some negative values which could occur in this implementation\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    # expanding the tree in all possible directions (all valid moves own a child node)\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                # we are always player 1 - we just change the states \n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    # update the whole tree (all parant nodes beginning from current node)\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    # this is the above explained sequence the mcts algorithm has to follow  \n",
    "    @torch.no_grad() # not updating the gradiants when calling this function => no training or updating the model here \n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        # the first pass-through is slightly different => adding noice (temperatur) to explore more\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        # deleting invalid moves and renormalizing before expanding\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # run until leaf node is found\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            # action_taken is here the action taken by the opponent => if value says we won -> opponent won\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminated:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "        # get the most promising child out of all root children\n",
    "        # => the child with the most visit_counts is the most promising \n",
    "        # because of how the ucb-formular works \n",
    "        # (after many simulations the one with the highest visit count usually \n",
    "        #   aligns with the action with the highest expected win rate)\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1c8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        # get policy of multiple states now\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        # expand only the root node of all spGames with an temperature\n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            # run through all self played games \n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                # backpropagate if game has ended in some way\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                    \n",
    "                else:\n",
    "                    # update node of object in list if game has not ended\n",
    "                    spg.node = node\n",
    "\n",
    "            # get all games which can still be expanded (node would be none if game has terminated)       \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            # expand all games which can be expanded and then backpropagate\n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "                \n",
    "            # backpropagate all self played games which haven't ended yet \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d60ef",
   "metadata": {},
   "source": [
    "## AlphaZero Implementation\n",
    "\n",
    "The AlphaZero algorithm works by looping following sequence n times:\n",
    "\n",
    "- collecting training data by playing itsself using the updated MTCS algorithm\n",
    "- train the neural network by the generated training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eeb694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "    \n",
    "    # one playthrough the game of choice\n",
    "    # returns a list containing: \n",
    "    #   - A encoded state of all states  \n",
    "    #   - the action probabilities to the correspondending states\n",
    "    #   - the player who won (1 or -1 -> starting player is 1)\n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            # change perspective of game field (has no effect in the first loop)\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            # get the probabilities of all valid moves with the mcts algorithm\n",
    "            action_probs = self.mcts.search(neutral_state) \n",
    "            # append current state and move probs to the memory of this game for later training\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            # temperature for more flexibility\n",
    "            # the bigger the args of temperature (the smaller temperature_action_probs) the more exploration \n",
    "            #   (choosing more often a way which has not been chosen as often before)  \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            # choose one move which has big probability of success\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs / np.sum(temperature_action_probs))\n",
    "            # make move on board\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            # check if game has ended and if player has won\n",
    "            value, is_terminated = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            # if playthrough is finished\n",
    "            if is_terminated:\n",
    "                returnMemory = [] # format the memory which will be used as training data\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    # value is always 1 if somebody won -> it must be -1 if opponent won \n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player) # switch player for next walkthrough\n",
    "                \n",
    "    # the training algorithm of the neural network \n",
    "    # the training data (memory) will be generated by the selfplay method\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory) # the memory is shuffled to prevent momorising \n",
    "        # training in batches\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1) # reshape(-1, 1): each value will be in its own subarray\n",
    "            \n",
    "            # make tensors for neural network\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            # run encoded state through the model\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            # run loss functions \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # run optimizer \n",
    "            self.optimizer.zero_grad() \n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    # the main loop where the training and the selfplay is initialized\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval() # set the model to eval mode\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                spgame = self.selfPlay()\n",
    "                memory += spgame\n",
    "                #np.save(\"games/game_{0}.npy\".format(selfPlay_iteration), np.array(spgame, dtype=object), allow_pickle=True)\n",
    "\n",
    "            self.model.train() # set the model to train mode\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model/model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer/optimizer_{iteration}_{self.game}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "790bfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]  # create empty game objects where the information will be stored\n",
    "        \n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames]) # create as many states as games in spGames\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "            \n",
    "            # go through all games in reverse order ([::-1])\n",
    "            # => reverse order because we delete terminated games from the list \n",
    "            #   => no alignment problems when doing in such way\n",
    "            for i in range(len(spGames))[::-1]: \n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                # add current state, the current visit count and the player to the memory \n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                # here / np.sum() because the action_probs has also been divided by its sum (=> stored as percentage in the spg.memory)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs / np.sum(temperature_action_probs))\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]  # deleting terminated game to make the while loop above working\n",
    "                    \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model/model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer/optimizer_{iteration}_{self.game}.pt\")\n",
    "\n",
    "# class for storing self played games information           \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3c027",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s8gre\\Documents\\PyTorchProjects\\AlphaZero\\AlphaZero.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                 data\u001b[39m.\u001b[39mappend((game\u001b[39m.\u001b[39mmainline_moves(), player))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m data \u001b[39m=\u001b[39m convert_data_chess(\u001b[39m\"\u001b[39;49m\u001b[39mmaster_games\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_move_to_probs\u001b[39m(game, state, move): \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(game\u001b[39m.\u001b[39maction_size)\n",
      "\u001b[1;32mc:\\Users\\s8gre\\Documents\\PyTorchProjects\\AlphaZero\\AlphaZero.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m pgn_game) \u001b[39mas\u001b[39;00m pgn_game_file: \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m: \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         game \u001b[39m=\u001b[39m chess\u001b[39m.\u001b[39;49mpgn\u001b[39m.\u001b[39;49mread_game(pgn_game_file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39mif\u001b[39;00m game \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/PyTorchProjects/AlphaZero/AlphaZero.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\chess\\pgn.py:1555\u001b[0m, in \u001b[0;36mread_game\u001b[1;34m(handle, Visitor)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m     \u001b[39m# Parse SAN tokens.\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m         move \u001b[39m=\u001b[39m visitor\u001b[39m.\u001b[39;49mparse_san(board_stack[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], token)\n\u001b[0;32m   1556\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m   1557\u001b[0m         visitor\u001b[39m.\u001b[39mhandle_error(error)\n",
      "File \u001b[1;32mc:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\chess\\pgn.py:938\u001b[0m, in \u001b[0;36mBaseVisitor.parse_san\u001b[1;34m(self, board, san)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[39melif\u001b[39;00m san \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0-0-0\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    936\u001b[0m     san \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mO-O-O\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m \u001b[39mreturn\u001b[39;00m board\u001b[39m.\u001b[39;49mparse_san(san)\n",
      "File \u001b[1;32mc:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\chess\\__init__.py:2822\u001b[0m, in \u001b[0;36mBoard.parse_san\u001b[1;34m(self, san)\u001b[0m\n\u001b[0;32m   2820\u001b[0m \u001b[39m# Match legal moves.\u001b[39;00m\n\u001b[0;32m   2821\u001b[0m matched_move \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2822\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_legal_moves(from_mask, to_mask):\n\u001b[0;32m   2823\u001b[0m     \u001b[39mif\u001b[39;00m move\u001b[39m.\u001b[39mpromotion \u001b[39m!=\u001b[39m promotion:\n\u001b[0;32m   2824\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\chess\\__init__.py:3332\u001b[0m, in \u001b[0;36mBoard.generate_legal_moves\u001b[1;34m(self, from_mask, to_mask)\u001b[0m\n\u001b[0;32m   3330\u001b[0m             \u001b[39myield\u001b[39;00m move\n\u001b[0;32m   3331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3332\u001b[0m     \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_pseudo_legal_moves(from_mask, to_mask):\n\u001b[0;32m   3333\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_safe(king, blockers, move):\n\u001b[0;32m   3334\u001b[0m             \u001b[39myield\u001b[39;00m move\n",
      "File \u001b[1;32mc:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\chess\\__init__.py:1543\u001b[0m, in \u001b[0;36mBoard.generate_pseudo_legal_moves\u001b[1;34m(self, from_mask, to_mask)\u001b[0m\n\u001b[0;32m   1540\u001b[0m     \u001b[39myield\u001b[39;00m Move(from_square, to_square)\n\u001b[0;32m   1542\u001b[0m \u001b[39m# Generate en passant captures.\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mep_square:\n\u001b[0;32m   1544\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_pseudo_legal_ep(from_mask, to_mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def convert_data_chess(path): \n",
    "    import chess.pgn, os\n",
    "    data = []\n",
    "    for pgn_game in os.listdir(path):\n",
    "        with open(path + \"/\" + pgn_game) as pgn_game_file: \n",
    "            while True: \n",
    "                game = chess.pgn.read_game(pgn_game_file)\n",
    "                if game is None: \n",
    "                    break\n",
    "                if game.headers.get(\"Result\", \"Unknown\") == \"1-0\": \n",
    "                    player = 1\n",
    "                elif game.headers.get(\"Result\", \"Unknown\") == \"0-1\":\n",
    "                    player = -1\n",
    "                else: \n",
    "                    player = 0 \n",
    "                data.append((game.mainline_moves(), player))\n",
    "    return data\n",
    "\n",
    "data = convert_data_chess(\"master_games\")\n",
    "\n",
    "def convert_move_to_probs(game, state, move): \n",
    "    probs = np.zeros(game.action_size)\n",
    "    probs[game.move_to_action(move, state)] = 1\n",
    "    return probs\n",
    "\n",
    "# TODO: pretty slow => training in batches!\n",
    "def train_existing_chess(model, optimizer, data):\n",
    "        for index, d in enumerate(data): \n",
    "            print(\"Game: \", index+1)\n",
    "            game = Chess()\n",
    "            state = game.get_initial_state()\n",
    "            player = 1        \n",
    "            model.train()\n",
    "            for move in d[0]: \n",
    "                state_target, policy_targets, value_targets = np.array(game.get_encoded_state(state)), np.array(convert_move_to_probs(game, state, move)), \\\n",
    "                    np.array(d[1]).reshape(-1, 1) # reshape(-1, 1): each value will be in its own subarray\n",
    "                \n",
    "                # make tensors for neural network\n",
    "                state_target = torch.tensor(state_target, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "                policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=model.device).unsqueeze(0)\n",
    "                value_targets = torch.tensor(value_targets, dtype=torch.float32, device=model.device)\n",
    "                # run encoded state through the model\n",
    "                out_policy, out_value = model(state_target)\n",
    "                \n",
    "                # run loss functions \n",
    "                policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "                value_loss = F.mse_loss(out_value, value_targets)\n",
    "                loss = policy_loss + value_loss\n",
    "                \n",
    "                # run optimizer \n",
    "                optimizer.zero_grad() \n",
    "                # backpropagate\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                player = game.get_opponent(player)\n",
    "                state = game.get_next_state(state, game.move_to_action(move, state), player)\n",
    "\n",
    "        torch.save(model.state_dict(), f\"model/model_pretrained.pt\")\n",
    "        torch.save(optimizer.state_dict(), f\"optimizer/optimizer_pretrained.pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet(Chess(), 12, 256, 18, device) # chess\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "train_existing_chess(model, optimizer, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24bd91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2a84fd0c384f27937f7036a08dab55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000002A34A02F8B0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\tqdm\\std.py\", line 1149, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\s8gre\\anaconda3\\envs\\ai\\lib\\site-packages\\tqdm\\notebook.py\", line 272, in close\n",
      "    if self.disable:\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disable'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec8d8d271d44bcb80f852f317e4efce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62b970bf47a4e9eb497a92ba6f2a3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69047861321842e3acd0a3e4d8a11574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b40767083b04ac5a4386fd38aedb24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8607e419ed4246a79a004ebb67ebbc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 4, 64, 3, device) # tictactoe\n",
    "# model = ResNet(game, 9, 128, 3, device) # fourconnect\n",
    "# model = ResNet(game, 12, 256, 18, device) # chess\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 50, # number which determines how many iterations the mcts algorithm should have\n",
    "    'num_iterations': 3, # number of looping iterations of the selfplay - train cycle\n",
    "    'num_selfPlay_iterations': 100, \n",
    "    'num_parallel_games': 150, # only needed if using parallel mode - currently not working for chess! \n",
    "    'num_epochs': 4, # number of epochs the model should train with the generated training data\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "# loading existing model and optimizer: \n",
    "# model.load_state_dict(torch.load(\"model/model_Chess.pt\"))\n",
    "# optimizer.load_state_dict(torch.load(\"optimizer/optimizer_Chess.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "# alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "\n",
    "# training the model on existing data / games\n",
    "# alphaZero.train()\n",
    "\n",
    "# run the alphazero algorithm \n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eae12b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c383e5279764dc090d617dde768a29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model with existing data: \n",
    "import os \n",
    "\n",
    "game = Chess()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = ResNet(game, 4, 64, 3, device) # tictactoe\n",
    "# model = ResNet(game, 9, 128, 3, device) # fourconnect\n",
    "model = ResNet(game, 12, 256, 18, device) # chess\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 125, # number which determines how many iterations the mcts algorithm should have\n",
    "    'num_iterations': 1, # number of looping iterations of the selfplay - train cycle\n",
    "    'num_selfPlay_iterations': 1, \n",
    "    'num_parallel_games': 150, \n",
    "    'num_epochs': 4, # number of epochs the model should train with the generated training data\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "alphaZero.model.train() # set the model to train mode\n",
    "training_data = []\n",
    "for file in os.listdir(\"games\"): \n",
    "    training_data.append(np.load(\"games/\" + file, allow_pickle=True))\n",
    "training_data = training_data[0]\n",
    "print(training_data.shape)\n",
    "for epoch in trange(alphaZero.args['num_epochs']):\n",
    "    alphaZero.train(training_data)\n",
    "\n",
    "#torch.save(alphaZero.model.state_dict(), f\"model/model_{alphaZero.game}.pt\")\n",
    "#torch.save(alphaZero.optimizer.state_dict(), f\"optimizer/optimizer_{alphaZero.game}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a4147",
   "metadata": {},
   "source": [
    "## Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a27e78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9300205111503601\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = game.get_initial_state()\n",
    "state = game.get_next_state(state, 2, -1)\n",
    "state = game.get_next_state(state, 4, -1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "state = game.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = game.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(game, 4, 64, 3, device=device)\n",
    "model.load_state_dict(torch.load('model/model_3_TicTacToe.pt', map_location=device))\n",
    "# model = ResNet(game, 9, 128, 3, device=device)\n",
    "# model.load_state_dict(torch.load('model/model_1_ConnectFour.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(game.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "991b7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.]\n",
      " [-1. -1.  1.]\n",
      " [ 1. -1.  1.]]\n",
      "[[ 1.  1. -1.]\n",
      " [-1. -1.  1.]\n",
      " [ 1. -1.  1.]]\n",
      "draw\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "game = TicTacToe()\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "\n",
    "}\n",
    "\n",
    "model = ResNet(game, 4, 64, 3, device)\n",
    "#model.load_state_dict(torch.load(\"model/model_3_TicTacToe.pt\", map_location=device))\n",
    "#model = ResNet(game, 9, 128, device)\n",
    "#model.load_state_dict(torch.load(\"model/model_1_ConnectFour.pt\", map_location=device))\n",
    "\n",
    "# model = ResNet(game, 12, 256, 18, device)\n",
    "#model.load_state_dict(torch.load(\"model/model_0_Chess.pt\", map_location=device))\n",
    "model.load_state_dict(torch.load(\"model/model_2_TicTacToe.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "        clear_output()\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    print(state)\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
