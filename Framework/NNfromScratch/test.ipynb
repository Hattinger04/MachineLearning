{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network from scratch\n",
    "\n",
    "Big shoutout to the book which guides this project: \"Neural Networks from Scratch in Python\" by Harrison Kinsley & Daniel Kukie≈Ça"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense: \n",
    "    def __init__(self, n_input, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_input, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): \n",
    "        # gradients on parameters for updating\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # gradients on values for next steps\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU: \n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues): \n",
    "        self.dinputs = dvalues.copy() # copy because of modifying \n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax: \n",
    "    def forward(self, inputs):\n",
    "        exp_value = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))  # subtracting max value for numerical stability => result will not change!\n",
    "        self.output = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid: \n",
    "    def forward(self, input): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent class\n",
    "class Loss: \n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss): \n",
    "    def forward(self, y_pred, y_true): \n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # to prevent 0 => log(0) is undefined\n",
    "\n",
    "        \"\"\"\n",
    "        example: \n",
    "        \n",
    "        y_pred_clipped = [[0.7, 0.1, 0.2],\n",
    "                          [0.1, 0.5, 0.4],\n",
    "                          [0.02, 0.9, 0.08]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # single dimension => categorical labels\n",
    "        if len(y_true.shape) == 1: # if is 1D-Array:\n",
    "            # e.g. y_true = [1, 0, 0]\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "    \n",
    "        # 2 dimensions => one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2: \n",
    "            # e.g. y_true = [[0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)            \n",
    "        else: \n",
    "            raise Exception(\"Please use a different shape for y_true: shape = {0}\".format(y_true.shape))\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true): \n",
    "        samples = len(dvalues) \n",
    "        labels = len(dvalues[0]) # number of labels in every sample\n",
    "\n",
    "        # ensure one-hot encoding\n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # calculate gradient \n",
    "        self.dinputs = -y_true / dvalues # derivative of loss function (-log(x)) => because of one-hot encoding the correct values get updated\n",
    "\n",
    "        # normalize gradient => optimizers sum all the gradients before multiplying them with the learning rate \n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333351 0.33333353 0.33333356 0.33333367 0.33333358\n",
      " 0.33333449 0.33333468 0.33333486 0.33333457 0.33333536 0.33333545\n",
      " 0.33333542 0.33333583 0.33333656 0.33333723 0.33333642 0.33333773\n",
      " 0.33333681 0.33334396 0.33334464 0.33333735 0.33334537 0.33334607\n",
      " 0.33334711 0.33334749 0.33334511 0.33334859 0.33334402 0.33334884\n",
      " 0.33334317 0.33335008 0.33335132 0.33335149 0.33335034 0.33333106\n",
      " 0.33333088 0.33333656 0.33333805 0.33334569 0.33333093 0.33335663\n",
      " 0.33333085 0.33333647 0.33332999 0.33332999 0.33333004 0.33332939\n",
      " 0.33332935 0.33332925 0.33332936 0.33333123 0.333329   0.33332906\n",
      " 0.33332939 0.33332907 0.33333587 0.33333505 0.3333285  0.33332856\n",
      " 0.33332876 0.33333468 0.33332814 0.33333391 0.33332882 0.33333559\n",
      " 0.33334435 0.33334778 0.33334208 0.33334619 0.33335646 0.3333463\n",
      " 0.33334615 0.33334874 0.33336135 0.33334578 0.33336157 0.33335402\n",
      " 0.33337454 0.33337845 0.3333768  0.33337944 0.33338009 0.33338047\n",
      " 0.33336392 0.33336314 0.33338151 0.33335012 0.33338389 0.3333837\n",
      " 0.33338423 0.33338553 0.33338603 0.33338267 0.33338703 0.33338382\n",
      " 0.33337723 0.33336624 0.33332862 0.33338538 0.33333333 0.3333332\n",
      " 0.33333288 0.33333285 0.33333284 0.33333251 0.3333331  0.33333372\n",
      " 0.3333335  0.33333411 0.3333342  0.33333427 0.33333404 0.33333416\n",
      " 0.33333405 0.33333429 0.33333463 0.33333327 0.33333482 0.33333495\n",
      " 0.33333491 0.33333515 0.33333267 0.33332971 0.3333328  0.33333531\n",
      " 0.33333146 0.33332966 0.33333484 0.33333454 0.33332931 0.33333592\n",
      " 0.33333186 0.33332828 0.33332829 0.33332798 0.33332764 0.33332377\n",
      " 0.33332796 0.33332363 0.33332435 0.33332318 0.3333224  0.33332194\n",
      " 0.33332338 0.33332503 0.33332117 0.3333234  0.33332045 0.33332155\n",
      " 0.33332052 0.3333208  0.33332088 0.33331932 0.33331887 0.33331856\n",
      " 0.33332008 0.33332595 0.33332489 0.33332999 0.33332207 0.33331901\n",
      " 0.33333043 0.33332329 0.33332791 0.33333854 0.33333315 0.33333095\n",
      " 0.33333749 0.33333871 0.33333828 0.33333643 0.33333887 0.33333775\n",
      " 0.3333385  0.33333982 0.33333967 0.33333142 0.33334016 0.33333922\n",
      " 0.33333926 0.33333973 0.33334039 0.33334041 0.33333918 0.3333402\n",
      " 0.3333257  0.33332472 0.33332552 0.33333711 0.33333791 0.3333254\n",
      " 0.33332469 0.33332179 0.33332173 0.33332197 0.33331868 0.33331832\n",
      " 0.33331776 0.33331747 0.33333333 0.33333305 0.33333306 0.33333303\n",
      " 0.33333274 0.33333204 0.33333314 0.33333263 0.33333075 0.33333103\n",
      " 0.33333089 0.33333027 0.33333065 0.33332948 0.33332916 0.3333284\n",
      " 0.3333289  0.33332775 0.33332982 0.33332762 0.33333097 0.33333022\n",
      " 0.33332651 0.33332733 0.33332564 0.33333182 0.33332495 0.3333326\n",
      " 0.33332636 0.33333324 0.33333311 0.33333239 0.33333325 0.33333322\n",
      " 0.33333226 0.3333304  0.3333332  0.33333321 0.33333322 0.33333323\n",
      " 0.33333319 0.33332964 0.33333321 0.33333318 0.33333318 0.33333318\n",
      " 0.33333316 0.33333315 0.33333232 0.3333325  0.33333315 0.33333261\n",
      " 0.33333157 0.33333253 0.33333163 0.33333152 0.33333142 0.33333232\n",
      " 0.33333136 0.33332057 0.33333136 0.33333134 0.33332442 0.33333129\n",
      " 0.33332456 0.33332071 0.33331271 0.33332666 0.33331152 0.33332322\n",
      " 0.33331154 0.33332914 0.3333146  0.33331086 0.33331797 0.3333098\n",
      " 0.33330887 0.33330931 0.33331067 0.33331023 0.33331822 0.33330799\n",
      " 0.33330875 0.3333061  0.33330652 0.33330618 0.33330961 0.33331404\n",
      " 0.33330566 0.33330409 0.33330616 0.33330757 0.3333331  0.33331632\n",
      " 0.3333331  0.33333309 0.33331739 0.33331826 0.33333302 0.33333296]\n",
      "Loss:  1.0986098781530163\n",
      "Acc:  0.38666666666666666\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)  \n",
    "layer1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output) \n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Acc: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
