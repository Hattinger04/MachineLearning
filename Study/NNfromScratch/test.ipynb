{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network from scratch\n",
    "\n",
    "Big shoutout to the book which guides this project: \"Neural Networks from Scratch in Python\" by Harrison Kinsley & Daniel KukieÅ‚a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense: \n",
    "    def __init__(self, n_input, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01 * np.random.randn(n_input, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # adding l1 and l2 regularization \n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): \n",
    "        # gradients on parameters for updating\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # backpropagating the l1 and l2 regularization \n",
    "        #   on weights \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        #   on biaeses \n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # gradients on values for next steps                \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # invert to get success rate \n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Dropout only helpful in training => not wanted when predicting \n",
    "        # To make that possible: scaling the data back by dividing self.rate to mimic the mean of the sum\n",
    "        #   => now in prediction the values are not (on average) bigger than when training \n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU: \n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues): \n",
    "        self.dinputs = dvalues.copy() # copy because of modifying \n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax: \n",
    "    def forward(self, inputs):\n",
    "        exp_value = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))  # subtracting max value for numerical stability => result will not change!\n",
    "        self.output = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # dinputs contains the gradiants\n",
    "        # in softmax the result is a jacobian matrix \n",
    "        # =>    calculating the partial derivatives of every ouput of the softmax output with respect to each input seperately \n",
    "        # =>    at the end, every batch will be summed with the np.dot function to not return a 3D array but 2D array\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid: \n",
    "    def forward(self, input): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent class\n",
    "class Loss: \n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "    # l1 and l2 regularization loss calculation on one layer \n",
    "    def regularization_loss(self, layer): \n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 *  np.sum(layer.biases * layer.biases)\n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss): \n",
    "    def forward(self, y_pred, y_true): \n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # to prevent 0 => log(0) is undefined\n",
    "\n",
    "        \"\"\"\n",
    "        example: \n",
    "        \n",
    "        y_pred_clipped = [[0.7, 0.1, 0.2],\n",
    "                          [0.1, 0.5, 0.4],\n",
    "                          [0.02, 0.9, 0.08]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # single dimension => categorical labels\n",
    "        if len(y_true.shape) == 1: # if is 1D-Array:\n",
    "            # e.g. y_true = [1, 0, 0]\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "    \n",
    "        # 2 dimensions => one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2: \n",
    "            # e.g. y_true = [[0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)            \n",
    "        else: \n",
    "            raise Exception(\"Please use a different shape for y_true: shape = {0}\".format(y_true.shape))\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true): \n",
    "        samples = len(dvalues) \n",
    "        labels = len(dvalues[0]) # number of labels in every sample\n",
    "\n",
    "        # ensure one-hot encoding\n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # calculate gradient \n",
    "        self.dinputs = -y_true / dvalues # derivative of loss function (-log(x)) => because of one-hot encoding the correct values get updated\n",
    "\n",
    "        # normalize gradient => optimizers sum all the gradients before multiplying them with the learning rate \n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of using softmax and categorical corssentropy loss functions we can simplify \n",
    "# the backpropagation and therefore greatly enhance our performance \n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(Loss):\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    def forward(self, inputs, y_true): \n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true): \n",
    "        samples = len(dvalues)\n",
    "        # if one-hot encoded turn into discrete values\n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) # axis 1: sample-wise performing \n",
    "        self.dinputs = dvalues.copy() # copy to not modify existing array\n",
    "        # calc gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradients \n",
    "        self.dinputs /= samples  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD is updating the weights and biases based on the momentum of the previous gradients \n",
    "#   and the new gradient\n",
    "class Optimizer_SGD(): \n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum: \n",
    "            # if layer does not contain momentum arrays => create it\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases) # if weights do not have momentum array then also biases\n",
    "            \n",
    "            # build weight update with momentum\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates # update momentum in layer for next update\n",
    "            # same for biases\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # optimizing gradients without momentum\n",
    "        else: \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights \n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # update weights and biases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad is very similar to SGD but it adds the idea of normalizing updates made to the featues\n",
    "#   it is not commonly used\n",
    "#   => in our example not as good as SGD!\n",
    "class Optimizer_Adagrad:\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp is very similiar to AdaGrad cache\n",
    "#   adds smoother learning rate changes \n",
    "#   uses moving average of cache instead of squared gradients like AdaGrad\n",
    "#   => in our example not as good as SGD!\n",
    "class Optimizer_RMSprop:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases /  (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam is a mix between SGD and RMSProp and combines the two principles \n",
    "#   meaning it applies momentum like SGD and then \n",
    "#   applies per-weight adaptive learning rate with the cache like RMSProp\n",
    "# Is the most used optimizer!\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / \\\n",
    "            (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / \\\n",
    "        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_progress(name, epoch, X, y, predict): \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                        np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    # Flatten the meshgrid points and make predictions\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = np.array([predict(x) for x in grid_points])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    colors = ['red', 'blue', 'green']\n",
    "\n",
    "\n",
    "    # Use a colormap for both the scatter plot and contourf plot\n",
    "    cmap = plt.cm.viridis # Choose the desired colormap\n",
    "\n",
    "    # Plot the decision boundaries using filled contours with the specified colormap\n",
    "    plt.contourf(xx, yy, Z, levels=np.arange(len(np.unique(y)) + 1) - 0.5, cmap=cmap, alpha=0.8)\n",
    "\n",
    "    # Scatter plot for the original data points with the same colormap\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolors='k', marker='.', s=40, alpha=0.8)\n",
    "\n",
    "    # Create a legend\n",
    "    legend_labels = np.unique(y)\n",
    "    legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=cmap(label)) for label in legend_labels]\n",
    "    plt.legend(handles=legend_handles, title='Classes')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"img/{name}_{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimized backward pass is 5.84487092210354 times faster!\n"
     ]
    }
   ],
   "source": [
    "# calculate the time difference when using the optimized \"activation_softmax_loss_categoricalcrossentropy\" way\n",
    "from timeit import timeit\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "def f1():\n",
    "    softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    softmax_loss.backward(softmax_outputs, class_targets)\n",
    "    dvalues1 = softmax_loss.dinputs\n",
    "   \n",
    "def f2():\n",
    "    activation = Activation_Softmax()\n",
    "    activation.output = softmax_outputs\n",
    "    loss = Loss_CategoricalCrossEntropy()\n",
    "    loss.backward(softmax_outputs, class_targets)\n",
    "    activation.backward(loss.dinputs)\n",
    "    dvalues2 = activation.dinputs\n",
    "    \n",
    "\n",
    "\n",
    "t1 = timeit(lambda: f1(), number=10000)\n",
    "t2 = timeit(lambda: f2(), number=10000)\n",
    "\n",
    "print(\"The optimized backward pass is {0} times faster!\".format(t2/t1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.309, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.02\n",
      "epoch: 200, acc: 0.766, loss: 0.728 (data_loss: 0.653, reg_loss: 0.074), lr: 0.019998010197985302\n",
      "epoch: 400, acc: 0.835, loss: 0.606 (data_loss: 0.509, reg_loss: 0.096), lr: 0.01999601079584623\n",
      "epoch: 600, acc: 0.847, loss: 0.554 (data_loss: 0.458, reg_loss: 0.096), lr: 0.01999401179346786\n",
      "epoch: 800, acc: 0.858, loss: 0.522 (data_loss: 0.430, reg_loss: 0.092), lr: 0.0199920131907303\n",
      "epoch: 1000, acc: 0.859, loss: 0.500 (data_loss: 0.411, reg_loss: 0.088), lr: 0.019990014987513734\n",
      "epoch: 1200, acc: 0.868, loss: 0.479 (data_loss: 0.393, reg_loss: 0.086), lr: 0.019988017183698373\n",
      "epoch: 1400, acc: 0.870, loss: 0.462 (data_loss: 0.377, reg_loss: 0.085), lr: 0.019986019779164473\n",
      "epoch: 1600, acc: 0.877, loss: 0.444 (data_loss: 0.361, reg_loss: 0.083), lr: 0.01998402277379235\n",
      "epoch: 1800, acc: 0.882, loss: 0.433 (data_loss: 0.351, reg_loss: 0.081), lr: 0.019982026167462367\n",
      "epoch: 2000, acc: 0.879, loss: 0.425 (data_loss: 0.346, reg_loss: 0.079), lr: 0.019980029960054924\n",
      "epoch: 2200, acc: 0.883, loss: 0.415 (data_loss: 0.338, reg_loss: 0.078), lr: 0.01997803415145048\n",
      "epoch: 2400, acc: 0.884, loss: 0.408 (data_loss: 0.332, reg_loss: 0.076), lr: 0.019976038741529537\n",
      "epoch: 2600, acc: 0.885, loss: 0.404 (data_loss: 0.329, reg_loss: 0.074), lr: 0.01997404373017264\n",
      "epoch: 2800, acc: 0.890, loss: 0.396 (data_loss: 0.324, reg_loss: 0.073), lr: 0.019972049117260395\n",
      "epoch: 3000, acc: 0.887, loss: 0.390 (data_loss: 0.319, reg_loss: 0.071), lr: 0.019970054902673444\n",
      "epoch: 3200, acc: 0.893, loss: 0.386 (data_loss: 0.316, reg_loss: 0.070), lr: 0.019968061086292475\n",
      "epoch: 3400, acc: 0.891, loss: 0.380 (data_loss: 0.311, reg_loss: 0.069), lr: 0.019966067667998237\n",
      "epoch: 3600, acc: 0.893, loss: 0.376 (data_loss: 0.309, reg_loss: 0.068), lr: 0.01996407464767152\n",
      "epoch: 3800, acc: 0.891, loss: 0.372 (data_loss: 0.306, reg_loss: 0.066), lr: 0.019962082025193145\n",
      "epoch: 4000, acc: 0.895, loss: 0.367 (data_loss: 0.302, reg_loss: 0.065), lr: 0.019960089800444013\n",
      "epoch: 4200, acc: 0.893, loss: 0.366 (data_loss: 0.302, reg_loss: 0.064), lr: 0.01995809797330505\n",
      "epoch: 4400, acc: 0.895, loss: 0.360 (data_loss: 0.297, reg_loss: 0.063), lr: 0.019956106543657228\n",
      "epoch: 4600, acc: 0.894, loss: 0.359 (data_loss: 0.296, reg_loss: 0.063), lr: 0.01995411551138158\n",
      "epoch: 4800, acc: 0.895, loss: 0.358 (data_loss: 0.296, reg_loss: 0.062), lr: 0.019952124876359174\n",
      "epoch: 5000, acc: 0.894, loss: 0.352 (data_loss: 0.292, reg_loss: 0.061), lr: 0.019950134638471142\n",
      "epoch: 5200, acc: 0.896, loss: 0.353 (data_loss: 0.294, reg_loss: 0.060), lr: 0.01994814479759864\n",
      "epoch: 5400, acc: 0.895, loss: 0.348 (data_loss: 0.289, reg_loss: 0.059), lr: 0.019946155353622895\n",
      "epoch: 5600, acc: 0.899, loss: 0.348 (data_loss: 0.287, reg_loss: 0.060), lr: 0.019944166306425162\n",
      "epoch: 5800, acc: 0.895, loss: 0.341 (data_loss: 0.282, reg_loss: 0.060), lr: 0.019942177655886757\n",
      "epoch: 6000, acc: 0.897, loss: 0.339 (data_loss: 0.281, reg_loss: 0.059), lr: 0.019940189401889033\n",
      "epoch: 6200, acc: 0.898, loss: 0.338 (data_loss: 0.280, reg_loss: 0.058), lr: 0.019938201544313403\n",
      "epoch: 6400, acc: 0.895, loss: 0.340 (data_loss: 0.283, reg_loss: 0.057), lr: 0.019936214083041307\n",
      "epoch: 6600, acc: 0.896, loss: 0.337 (data_loss: 0.281, reg_loss: 0.057), lr: 0.019934227017954262\n",
      "epoch: 6800, acc: 0.897, loss: 0.335 (data_loss: 0.279, reg_loss: 0.056), lr: 0.0199322403489338\n",
      "epoch: 7000, acc: 0.899, loss: 0.332 (data_loss: 0.277, reg_loss: 0.055), lr: 0.019930254075861523\n",
      "epoch: 7200, acc: 0.898, loss: 0.332 (data_loss: 0.277, reg_loss: 0.055), lr: 0.01992826819861907\n",
      "epoch: 7400, acc: 0.899, loss: 0.331 (data_loss: 0.277, reg_loss: 0.054), lr: 0.019926282717088132\n",
      "epoch: 7600, acc: 0.896, loss: 0.331 (data_loss: 0.277, reg_loss: 0.054), lr: 0.019924297631150445\n",
      "epoch: 7800, acc: 0.896, loss: 0.330 (data_loss: 0.277, reg_loss: 0.053), lr: 0.01992231294068779\n",
      "epoch: 8000, acc: 0.899, loss: 0.326 (data_loss: 0.273, reg_loss: 0.052), lr: 0.019920328645582\n",
      "epoch: 8200, acc: 0.895, loss: 0.328 (data_loss: 0.276, reg_loss: 0.052), lr: 0.019918344745714942\n",
      "epoch: 8400, acc: 0.900, loss: 0.326 (data_loss: 0.275, reg_loss: 0.051), lr: 0.019916361240968555\n",
      "epoch: 8600, acc: 0.898, loss: 0.325 (data_loss: 0.274, reg_loss: 0.051), lr: 0.019914378131224802\n",
      "epoch: 8800, acc: 0.901, loss: 0.320 (data_loss: 0.270, reg_loss: 0.050), lr: 0.0199123954163657\n",
      "epoch: 9000, acc: 0.898, loss: 0.322 (data_loss: 0.272, reg_loss: 0.050), lr: 0.019910413096273318\n",
      "epoch: 9200, acc: 0.899, loss: 0.318 (data_loss: 0.268, reg_loss: 0.049), lr: 0.019908431170829768\n",
      "epoch: 9400, acc: 0.902, loss: 0.317 (data_loss: 0.268, reg_loss: 0.049), lr: 0.01990644963991721\n",
      "epoch: 9600, acc: 0.901, loss: 0.316 (data_loss: 0.267, reg_loss: 0.048), lr: 0.019904468503417844\n",
      "epoch: 9800, acc: 0.900, loss: 0.314 (data_loss: 0.266, reg_loss: 0.048), lr: 0.019902487761213932\n",
      "epoch: 10000, acc: 0.901, loss: 0.314 (data_loss: 0.266, reg_loss: 0.047), lr: 0.019900507413187767\n",
      "validation, acc: 0.887, loss: 0.272\n"
     ]
    }
   ],
   "source": [
    "# Create dataset    \n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "\n",
    "# Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 512, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "dense2 = Layer_Dense(512, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "\n",
    "softmax = Activation_Softmax()\n",
    "\n",
    "def predict_class(x): \n",
    "    dense1.forward(x)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    softmax.forward(dense2.output)\n",
    "    return np.argmax(softmax.output, axis=1)\n",
    "\n",
    "\n",
    "# loop the training process\n",
    "epochs = 10001\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    # calculating and adding the regularization loss of each layer\n",
    "    regularization_loss = loss_activation.regularization_loss(dense1) + loss_activation.regularization_loss(dense2)\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 200:\n",
    "        print(\n",
    "            f'epoch: {epoch}, ' +\n",
    "            f'acc: {accuracy:.3f}, ' +\n",
    "            f'loss: {loss:.3f} (' +\n",
    "            f'data_loss: {data_loss:.3f}, ' +\n",
    "            f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "            f'lr: {optimizer.current_learning_rate}')\n",
    "        # if wanted: plotting the learning progress of the network \n",
    "        # plot_progress(\"spiral_data\", epoch, X, y, predict_class)\n",
    "\n",
    "\n",
    "\n",
    "    # update params based on calculated gradients\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# validation of our model: \n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
