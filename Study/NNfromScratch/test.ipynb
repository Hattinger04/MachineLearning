{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network from scratch\n",
    "\n",
    "Big shoutout to the book which guides this project: \"Neural Networks from Scratch in Python\" by Harrison Kinsley & Daniel Kukie≈Ça"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense: \n",
    "    def __init__(self, n_input, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_input, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues): \n",
    "        # gradients on parameters for updating\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # gradients on values for next steps\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU: \n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues): \n",
    "        self.dinputs = dvalues.copy() # copy because of modifying \n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax: \n",
    "    def forward(self, inputs):\n",
    "        exp_value = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))  # subtracting max value for numerical stability => result will not change!\n",
    "        self.output = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # dinputs contains the gradiants\n",
    "        # in softmax the result is a jacobian matrix \n",
    "        # =>    calculating the partial derivatives of every ouput of the softmax output with respect to each input seperately \n",
    "        # =>    at the end, every batch will be summed with the np.dot function to not return a 3D array but 2D array\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid: \n",
    "    def forward(self, input): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent class\n",
    "class Loss: \n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss): \n",
    "    def forward(self, y_pred, y_true): \n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) # to prevent 0 => log(0) is undefined\n",
    "\n",
    "        \"\"\"\n",
    "        example: \n",
    "        \n",
    "        y_pred_clipped = [[0.7, 0.1, 0.2],\n",
    "                          [0.1, 0.5, 0.4],\n",
    "                          [0.02, 0.9, 0.08]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # single dimension => categorical labels\n",
    "        if len(y_true.shape) == 1: # if is 1D-Array:\n",
    "            # e.g. y_true = [1, 0, 0]\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "    \n",
    "        # 2 dimensions => one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2: \n",
    "            # e.g. y_true = [[0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)            \n",
    "        else: \n",
    "            raise Exception(\"Please use a different shape for y_true: shape = {0}\".format(y_true.shape))\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true): \n",
    "        samples = len(dvalues) \n",
    "        labels = len(dvalues[0]) # number of labels in every sample\n",
    "\n",
    "        # ensure one-hot encoding\n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # calculate gradient \n",
    "        self.dinputs = -y_true / dvalues # derivative of loss function (-log(x)) => because of one-hot encoding the correct values get updated\n",
    "\n",
    "        # normalize gradient => optimizers sum all the gradients before multiplying them with the learning rate \n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of using softmax and categorical corssentropy loss functions we can simplify \n",
    "# the backpropagation and therefore greatly enhance our performance \n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    def forward(self, inputs, y_true): \n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true): \n",
    "        samples = len(dvalues)\n",
    "        # if one-hot encoded turn into discrete values\n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) # axis 1: sample-wise performing \n",
    "        self.dinputs = dvalues.copy() # copy to not modify existing array\n",
    "        # calc gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradients \n",
    "        self.dinputs /= samples  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD(): \n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum: \n",
    "            # if layer does not contain momentum arrays => create it\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases) # if weights do not have momentum array then also biases\n",
    "            \n",
    "            # build weight update with momentum\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates # update momentum in layer for next update\n",
    "            # same for biases\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # optimizing gradients without momentum\n",
    "        else: \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights \n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # update weights and biases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_progress(epoch, name, predict): \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                        np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    # Flatten the meshgrid points and make predictions\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = np.array([predict(x) for x in grid_points])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    colors = ['red', 'blue', 'green']\n",
    "\n",
    "\n",
    "    # Use a colormap for both the scatter plot and contourf plot\n",
    "    cmap = plt.cm.viridis # Choose the desired colormap\n",
    "\n",
    "    # Plot the decision boundaries using filled contours with the specified colormap\n",
    "    plt.contourf(xx, yy, Z, levels=np.arange(len(np.unique(y)) + 1) - 0.5, cmap=cmap, alpha=0.8)\n",
    "\n",
    "    # Scatter plot for the original data points with the same colormap\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolors='k', marker='.', s=40, alpha=0.8)\n",
    "\n",
    "    # Create a legend\n",
    "    legend_labels = np.unique(y)\n",
    "    legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=cmap(label)) for label in legend_labels]\n",
    "    plt.legend(handles=legend_handles, title='Classes')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"img/{name}_{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimized backward pass is 4.319570925691128 times faster!\n"
     ]
    }
   ],
   "source": [
    "# calculate the time difference when using the optimized \"activation_softmax_loss_categoricalcrossentropy\" way\n",
    "from timeit import timeit\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "def f1():\n",
    "    softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    softmax_loss.backward(softmax_outputs, class_targets)\n",
    "    dvalues1 = softmax_loss.dinputs\n",
    "   \n",
    "def f2():\n",
    "    activation = Activation_Softmax()\n",
    "    activation.output = softmax_outputs\n",
    "    loss = Loss_CategoricalCrossEntropy()\n",
    "    loss.backward(softmax_outputs, class_targets)\n",
    "    activation.backward(loss.dinputs)\n",
    "    dvalues2 = activation.dinputs\n",
    "    \n",
    "\n",
    "\n",
    "t1 = timeit(lambda: f1(), number=10000)\n",
    "t2 = timeit(lambda: f2(), number=10000)\n",
    "\n",
    "print(\"The optimized backward pass is {0} times faster!\".format(t2/t1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.307, loss: 1.099 lr: 1.0\n",
      "epoch: 100, acc: 0.480, loss: 1.029 lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.477, loss: 1.021 lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.697, loss: 0.738 lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.787, loss: 0.528 lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.863, loss: 0.379 lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.890, loss: 0.318 lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.897, loss: 0.271 lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.910, loss: 0.245 lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.920, loss: 0.226 lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.923, loss: 0.211 lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.933, loss: 0.198 lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.937, loss: 0.188 lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.940, loss: 0.179 lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.940, loss: 0.172 lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.940, loss: 0.166 lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.943, loss: 0.161 lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.943, loss: 0.156 lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.943, loss: 0.153 lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.943, loss: 0.149 lr: 0.3449465332873405\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s8gre\\Documents\\MachineLearning\\Study\\NNfromScratch\\test.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10001\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs): \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     dense1\u001b[39m.\u001b[39;49mforward(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     activation1\u001b[39m.\u001b[39mforward(dense1\u001b[39m.\u001b[39moutput)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     dense2\u001b[39m.\u001b[39mforward(activation1\u001b[39m.\u001b[39moutput)\n",
      "\u001b[1;32mc:\\Users\\s8gre\\Documents\\MachineLearning\\Study\\NNfromScratch\\test.ipynb Cell 19\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs): \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/s8gre/Documents/MachineLearning/Study/NNfromScratch/test.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbiases\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create dataset    \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "\n",
    "softmax = Activation_Softmax()\n",
    "\n",
    "def predict_class(x): \n",
    "    dense1.forward(x)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    softmax.forward(dense2.output)\n",
    "    return np.argmax(softmax.output, axis=1)\n",
    "\n",
    "\n",
    "# loop the training process\n",
    "epochs = 10001\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 200:\n",
    "        print(\n",
    "            f'epoch: {epoch}, ' +\n",
    "            f'acc: {accuracy:.3f}, ' +\n",
    "            f'loss: {loss:.3f}', \n",
    "            f'lr: {optimizer.current_learning_rate}')\n",
    "        plot_progress(\"spiral_data\", epoch, predict_class)\n",
    "\n",
    "\n",
    "\n",
    "    # update params based on calculated gradients\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
