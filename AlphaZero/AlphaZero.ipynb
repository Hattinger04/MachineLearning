{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9ec190",
   "metadata": {},
   "source": [
    "# Alpha Zero\n",
    "This project is about developing my own chess AI by using PyTorch and Robert FÃ¶rster's course on YouTube\n",
    "(https://www.youtube.com/watch?v=wuSQpLinRB4) which has been a very big help. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88d1e8",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd6456",
   "metadata": {},
   "source": [
    "## Create a game\n",
    "\n",
    "The game must have following  properties / methods: \n",
    "- <b>row_count:</b> return the number of rows of the board\n",
    "- <b>column_count:</b> return the number of columns of the board\n",
    "- <b>action_size:</b> return the actions which can be taken at the board (TicTacToe: row_count * column_count)  \n",
    "- <b>get_initial_state():</b> return the board at the beginning of the game\n",
    "- <b>get_next_state(state, action, player):</b> make a move and update the board\n",
    "- <b>get_valid_moves(state):</b> return all possible moves at the current state\n",
    "- <b>check_win(state, action):</b> check if an action at a current state results in a win\n",
    "- <b>get_value_and_terminated(state, action):</b> check if the game has ended in some way (lose, draw, win)\n",
    "- <b>get_opponent(player):</b> get the opponent player \n",
    "- <b>get_opponent_value(player):</b> get the value of the opponent player \n",
    "- <b>change_perspective(state, player):</b> change the perspective of the board (e.g. state * player)\n",
    "- <b>get_encoded_state(state):</b> return a encoded version of the current state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True # this action ended the game with a win for the player who took this action\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True # this action ended the game but the players drew\n",
    "        return 0, False # the game has not ended yet\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c8b50",
   "metadata": {},
   "source": [
    "## Designing of a model\n",
    "\n",
    "Here a ResNet model has been chosen, containing of multiple Conv2D Layers which can find patterns in 2d games like TicTacToe or chess.  \n",
    "The ResNet has following layers:\n",
    "- 1 x startblock \n",
    "- n x backbone (here the input will be additionally added to the output)\n",
    "- 1 x policyhead (returns policy values as a linear layer)\n",
    "- 1 x valuehead (returns value in range -1 and 1 (by using tanh))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        # combination of kernel_size=3, padding=1 and stride=1 important => guarantees that input_shape = output_shape  \n",
    "        # e.g.: 5x5 image with 3x3 convolution results in a 3x3 output => padding 1 in all directions results in a 5x5 output\n",
    "        # therefore the shape of the image stays consistant during the computations\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a7539",
   "metadata": {},
   "source": [
    "## Updated MCTS Implementation\n",
    "\n",
    "Using the Monte Carlo Tree Search algorithm enables splitting the game of choice in many different paths of action and their probability of success.  \n",
    "Therefore it is perfect for solving a game tree in games like chess or TicTacToe.  \n",
    "It contains of four phases: \n",
    "- <u>Selection:</u> Walking down until a leaf node has been found (leaf node is a node that could be expanded even further)  \n",
    "             The path is determined by the UCB-fomular (a node is chosen which has a higher winning prob and lower visit counts)\n",
    "- <u>Expansion:</u> When reached a leaf node in the selection phase, new leaf node(s) will be created, which are attached at the current node\n",
    "- <u>Simulation:</u> Simulate the game by either random choices or like in the updated version, by using a neural network \n",
    "- <u>Backpropagation:</u> Update the tree from the bottom to the top (until root node is reached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b31e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state # current state of the game\n",
    "        self.parent = parent # the parent node\n",
    "        self.action_taken = action_taken # the action which has to be taken to arrive at this node \n",
    "        self.prior = prior # is the policy value the model provided us \n",
    "        \n",
    "        self.children = [] # list of all child nodes\n",
    "        \n",
    "        self.visit_count = visit_count # the count of all visits to this node\n",
    "        self.value_sum = 0 # the current sum of all simulations (of the neural network)\n",
    "    \n",
    "    # check if current node has child nodes\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    # returns the child node with the highest ucb score during selection phase\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    # calculate the ucb value of a child node\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # 1 - ()  because we want to have the opponent in the worst possible situation in the next move\n",
    "            # ( +1) / 2 because we want a proablilty and not some negative values which could occur in this implementation\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    # expanding the tree in all possible directions (all valid moves own a child node)\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                # we are always player 1 - we just change the states \n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    # update the whole tree (all parant nodes beginning from current node)\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    # this is the above explained sequence the mcts algorithm has to follow  \n",
    "    @torch.no_grad() # not updating the gradiants when calling this function => no training or updating the model here \n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        # deleting invalid moves and renormalizing before expanding\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # run until leaf node is found\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminated = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            # action_taken is here the action taken by the opponent => if value says we won -> opponent won\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminated:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "        # get the most promising child out of all root children\n",
    "        # => the child with the most visit_counts is the most promising \n",
    "        # because of how the ucb-formular works\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d60ef",
   "metadata": {},
   "source": [
    "## AlphaZero Implementation\n",
    "The AlphaZero algorithm works by looping following sequence n times: \n",
    "- let the engine play itsself by using the updated MTCS algorithm -> collecting training data\n",
    "- update the engine by its generated training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eeb694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "    \n",
    "    # one playthrough the game of choice\n",
    "    # returns a list containing: \n",
    "    #   - A encoded state of all states  \n",
    "    #   - the action probabilities to the correspondending states\n",
    "    #   - the player who won (1 or -1 -> starting player is 1)\n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            # change perspective of game field (has no effect in the first loop)\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            # get the probabilities of all valid moves with the mcts algorithm\n",
    "            action_probs = self.mcts.search(neutral_state) \n",
    "            # append current state and move probs to the memory of this game for later training\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            # choose one move which has big probability of success\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "            # make move on board\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            # check if game has ended and if player has won\n",
    "            value, is_terminated = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            # if playthrough is finished\n",
    "            if is_terminated:\n",
    "                returnMemory = [] # format the memory which will be used as training data\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    # value is always 1 if somebody won -> it must be -1 if opponent won \n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player) # switch player for next walkthrough\n",
    "                \n",
    "    # the training algorithm of the neural network \n",
    "    # the training data (memory) will be generated by the selfplay method\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory) # the memory is shuffled to prevent momorising \n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            # make tensors for neural network\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            # run encoded state through the model\n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            # run loss functions \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # run optimizer \n",
    "            self.optimizer.zero_grad() \n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    # the main loop where the training and the selfplay is initialized\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval() # set the model to eval mode\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train() # set the model to train mode\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3c027",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a4147",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27e78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939461886882782\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991b7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "probs:  [0.    0.07  0.098 0.088 0.179 0.125 0.223 0.067 0.15 ]\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [-1.  0.  0.]]\n",
      "valid_moves [1, 2, 3, 4, 5, 7, 8]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m valid_moves \u001b[39m=\u001b[39m tictactoe\u001b[39m.\u001b[39mget_valid_moves(state)\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvalid_moves\u001b[39m\u001b[39m\"\u001b[39m, [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tictactoe\u001b[39m.\u001b[39maction_size) \u001b[39mif\u001b[39;00m valid_moves[i] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mplayer\u001b[39m}\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m valid_moves[action] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maction not valid\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
